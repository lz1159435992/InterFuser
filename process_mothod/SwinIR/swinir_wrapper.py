"""
SwinIR åŒ…è£…å™¨ - ç”¨äº InterFuser æ•°æ®å¤„ç†å™¨é›†æˆ

æä¾›ç®€å•çš„æ¥å£æ¥ä½¿ç”¨ SwinIR è¿›è¡Œå›¾åƒå¤„ç†
æ”¯æŒçš„ä»»åŠ¡ï¼š
  - sr: è¶…åˆ†è¾¨ç‡
  - denoise: å»å™ª
  - jpeg: JPEG å‹ç¼©ä¼ªå½±å»é™¤
"""

import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼ˆswinir_wrapper.py ç°åœ¨åœ¨ SwinIR/ å†…éƒ¨ï¼‰
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
if CURRENT_DIR not in sys.path:
    sys.path.insert(0, CURRENT_DIR)

import torch
import numpy as np
import cv2

try:
    from .models.network_swinir import SwinIR
except ImportError as e:
    print(f"âš ï¸  æ— æ³•å¯¼å…¥ SwinIR: {e}")
    print(f"è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•: {CURRENT_DIR}")
    print(f"swinir_wrapper.py åº”è¯¥åœ¨ SwinIR/ æ–‡ä»¶å¤¹å†…éƒ¨")
    raise


class SwinIRProcessor:
    """
    SwinIR å›¾åƒå¤„ç†å™¨åŒ…è£…ç±»
    
    ä½¿ç”¨ç¤ºä¾‹:
        processor = SwinIRProcessor(
            model_path='model_zoo/swinir/xxx.pth',
            task='sr',
            upscale=2
        )
        
        output = processor.process(input_image)
    """
    
    # é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®
    MODEL_CONFIGS = {
        'sr_x2': {
            'upscale': 2,
            'in_chans': 3,
            'img_size': 64,
            'window_size': 8,
            'img_range': 1.,
            'depths': [6, 6, 6, 6],
            'embed_dim': 60,
            'num_heads': [6, 6, 6, 6],
            'mlp_ratio': 2,
            'upsampler': 'pixelshuffledirect',
            'resi_connection': '1conv'
        },
        'sr_x4': {
            'upscale': 4,
            'in_chans': 3,
            'img_size': 64,
            'window_size': 8,
            'img_range': 1.,
            'depths': [6, 6, 6, 6, 6, 6],
            'embed_dim': 180,
            'num_heads': [6, 6, 6, 6, 6, 6],
            'mlp_ratio': 2,
            'upsampler': 'pixelshuffle',
            'resi_connection': '1conv'
        },
        'denoise': {
            'upscale': 1,
            'in_chans': 3,
            'img_size': 128,
            'window_size': 8,
            'img_range': 1.,
            'depths': [6, 6, 6, 6, 6, 6],
            'embed_dim': 180,
            'num_heads': [6, 6, 6, 6, 6, 6],
            'mlp_ratio': 2,
            'upsampler': '',
            'resi_connection': '1conv'
        },
        'jpeg': {
            'upscale': 1,
            'in_chans': 3,
            'img_size': 126,
            'window_size': 7,
            'img_range': 255.,
            'depths': [6, 6, 6, 6, 6, 6],
            'embed_dim': 180,
            'num_heads': [6, 6, 6, 6, 6, 6],
            'mlp_ratio': 2,
            'upsampler': '',
            'resi_connection': '1conv'
        }
    }
    
    def __init__(self, model_path, task='color_dn', upscale=1, device='cuda', half_precision=False,
                 noise=15, jpeg=40, training_patch_size=128, tile=None, tile_overlap=32):
        """
        åˆå§‹åŒ– SwinIR å¤„ç†å™¨
        
        Args:
            model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
            task: ä»»åŠ¡ç±»å‹
                  - 'classical_sr': ç»å…¸è¶…åˆ†è¾¨ç‡
                  - 'lightweight_sr': è½»é‡çº§è¶…åˆ†è¾¨ç‡
                  - 'real_sr': çœŸå®ä¸–ç•Œè¶…åˆ†è¾¨ç‡
                  - 'gray_dn': ç°åº¦å›¾åƒå»å™ª
                  - 'color_dn': å½©è‰²å›¾åƒå»å™ª (é»˜è®¤)
                  - 'jpeg_car': JPEG å‹ç¼©ä¼ªå½±å»é™¤
                  - 'color_jpeg_car': å½©è‰² JPEG å‹ç¼©ä¼ªå½±å»é™¤
                  - 'sr' or 'sr_x2': 2x è¶…åˆ†è¾¨ç‡ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
                  - 'sr_x4': 4x è¶…åˆ†è¾¨ç‡ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
            upscale: æ”¾å¤§å€æ•°ï¼ˆSR ä»»åŠ¡ä½¿ç”¨ï¼Œé»˜è®¤ 1ï¼‰
            device: 'cuda' æˆ– 'cpu'
            half_precision: æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦ï¼ˆFP16ï¼‰
            noise: å™ªå£°ç­‰çº§ï¼Œç”¨äºå»å™ªä»»åŠ¡ (15, 25, 50)ï¼Œé»˜è®¤ 15
            jpeg: JPEG è´¨é‡ï¼Œç”¨äº JPEG ä¿®å¤ä»»åŠ¡ (10, 20, 30, 40)ï¼Œé»˜è®¤ 40
            training_patch_size: è®­ç»ƒæ—¶ä½¿ç”¨çš„ patch å¤§å°ï¼Œé»˜è®¤ 128
            tile: ç“¦ç‰‡å¤§å°ï¼ŒNone è¡¨ç¤ºæ•´å›¾å¤„ç†ï¼Œé»˜è®¤ None
            tile_overlap: ç“¦ç‰‡é‡å å¤§å°ï¼Œé»˜è®¤ 32
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        if self.device == 'cuda' and not torch.cuda.is_available():
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
            self.device = 'cpu'
        
        self.task = task
        self.upscale = upscale
        self.half_precision = half_precision and self.device == 'cuda'
        self.noise = noise
        self.jpeg = jpeg
        self.training_patch_size = training_patch_size
        self.tile = tile
        self.tile_overlap = tile_overlap
        
        # æ ¹æ®ä»»åŠ¡é€‰æ‹©é…ç½®
        if task in ['sr', 'sr_x2', 'classical_sr', 'lightweight_sr', 'real_sr']:
            if upscale == 2:
                config_key = 'sr_x2'
            elif upscale == 4:
                config_key = 'sr_x4'
            else:
                config_key = 'sr_x2'  # é»˜è®¤
        elif task in ['denoise', 'gray_dn', 'color_dn']:
            config_key = 'denoise'
        elif task in ['jpeg', 'jpeg_car', 'color_jpeg_car']:
            config_key = 'jpeg'
        else:
            raise ValueError(f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {task}. æ”¯æŒ: classical_sr, lightweight_sr, real_sr, "
                           f"gray_dn, color_dn, jpeg_car, color_jpeg_car, sr, sr_x2, sr_x4")
        
        model_config = self.MODEL_CONFIGS[config_key].copy()
        
        # å¯¹äº SR ä»»åŠ¡ï¼Œä½¿ç”¨æŒ‡å®šçš„ upscale
        if 'sr' in task:
            model_config['upscale'] = upscale
        
        # åˆ›å»ºæ¨¡å‹
        print(f"ğŸ“¦ åˆ›å»º SwinIR æ¨¡å‹: ä»»åŠ¡={task}, é…ç½®={config_key}")
        self.model = SwinIR(**model_config)
        
        # åŠ è½½æƒé‡
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        try:
            pretrained = torch.load(model_path, map_location='cpu')
            
            # å¤„ç†ä¸åŒçš„æƒé‡æ ¼å¼
            if 'params' in pretrained:
                state_dict = pretrained['params']
            elif 'model' in pretrained:
                state_dict = pretrained['model']
            else:
                state_dict = pretrained
            
            self.model.load_state_dict(state_dict, strict=True)
            print("âœ… æƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.model = self.model.to(self.device)
        
        # åŠç²¾åº¦
        if self.half_precision:
            self.model = self.model.half()
            print("âš¡ å¯ç”¨åŠç²¾åº¦ (FP16)")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.process_count = 0
        
        print(f"âœ… SwinIR åˆå§‹åŒ–å®Œæˆ")
        print(f"   ä»»åŠ¡: {task}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   æ”¾å¤§å€æ•°: {model_config['upscale']}")
        if task in ['denoise', 'gray_dn', 'color_dn']:
            print(f"   å™ªå£°ç­‰çº§: {noise}")
        if task in ['jpeg', 'jpeg_car', 'color_jpeg_car']:
            print(f"   JPEG è´¨é‡: {jpeg}")
        if tile is not None:
            print(f"   ç“¦ç‰‡å¤§å°: {tile}")
            print(f"   ç“¦ç‰‡é‡å : {tile_overlap}")
    
    def process(self, image_np):
        """
        å¤„ç†å•å¼ å›¾åƒ
        
        Args:
            image_np: numpy array, shape (H, W, 3), RGB, 0-255
        
        Returns:
            processed_image: numpy array, shape (H', W', 3), RGB, 0-255
                            å¯¹äº SR: H'=H*upscale, W'=W*upscale
                            å¯¹äºå…¶ä»–ä»»åŠ¡: H'=H, W'=W
        """
        if not isinstance(image_np, np.ndarray):
            raise TypeError(f"è¾“å…¥å¿…é¡»æ˜¯ numpy arrayï¼Œå®é™…ç±»å‹: {type(image_np)}")
        
        if image_np.ndim != 3 or image_np.shape[2] != 3:
            raise ValueError(f"è¾“å…¥å›¾åƒå¿…é¡»æ˜¯ (H, W, 3)ï¼Œå®é™…å½¢çŠ¶: {image_np.shape}")
        
        # é¢„å¤„ç†
        img = image_np.astype(np.float32)
        
        # æ ¹æ®ä»»åŠ¡è°ƒæ•´è¾“å…¥èŒƒå›´
        if self.task == 'jpeg':
            # JPEG ä»»åŠ¡ä½¿ç”¨ 0-255 èŒƒå›´
            img_range = 255.0
        else:
            img_range = 1.0
            img = img / 255.0
        
        # è½¬æ¢ä¸º tensor: (H, W, 3) -> (1, 3, H, W)
        img_tensor = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0)
        img_tensor = img_tensor.to(self.device)
        
        if self.half_precision:
            img_tensor = img_tensor.half()
        
        def _forward_with_tile(x, tile_size, overlap):
            b, c, h, w = x.shape
            stride = tile_size - overlap
            # Determine scale factor
            if self.task in ['denoise', 'gray_dn', 'color_dn', 'jpeg', 'jpeg_car', 'color_jpeg_car']:
                s = 1  # no spatial scaling for denoise/jpeg tasks
            else:
                # For SR tasks, detect scale once using a small center crop
                yc = min(tile_size, h)
                xc = min(tile_size, w)
                y0c = (h - yc) // 2
                x0c = (w - xc) // 2
                with torch.no_grad():
                    test_out = self.model(x[:, :, y0c:y0c+yc, x0c:x0c+xc])
                sh = test_out.shape[-2] // yc
                s = max(1, sh)  # assume isotropic scale
            Hs, Ws = h * s, w * s
            out = torch.zeros((b, c, Hs, Ws), device=x.device, dtype=x.dtype)
            wei = torch.zeros((b, c, Hs, Ws), device=x.device, dtype=x.dtype)
            ys = list(range(0, max(h - tile_size + stride, 1), stride)) if h > tile_size else [0]
            xs = list(range(0, max(w - tile_size + stride, 1), stride)) if w > tile_size else [0]
            for y in ys:
                y0 = min(y, max(h - tile_size, 0))
                y1 = min(y0 + tile_size, h)
                for x_ in xs:
                    x0 = min(x_, max(w - tile_size, 0))
                    x1 = min(x0 + tile_size, w)
                    patch = x[:, :, y0:y1, x0:x1]
                    ph, pw = patch.shape[-2], patch.shape[-1]
                    with torch.no_grad():
                        out_patch = self.model(patch)
                    Y0 = y0 * s
                    X0 = x0 * s
                    Y1 = Y0 + ph * s
                    X1 = X0 + pw * s
                    out[:, :, Y0:Y1, X0:X1] += out_patch
                    wei[:, :, Y0:Y1, X0:X1] += 1
            wei = wei.clamp(min=1)
            return out / wei

        with torch.no_grad():
            if self.tile is not None and (img_tensor.shape[-2] > int(self.tile) or img_tensor.shape[-1] > int(self.tile)):
                output_tensor = _forward_with_tile(img_tensor, int(self.tile), int(self.tile_overlap))
            else:
                try:
                    output_tensor = self.model(img_tensor)
                except RuntimeError as e:
                    if 'out of memory' in str(e).lower() and self.device == 'cuda':
                        try:
                            torch.cuda.empty_cache()
                        except Exception:
                            pass
                        output_tensor = _forward_with_tile(img_tensor, 512, 32)
                    else:
                        raise
        
        # åå¤„ç†: (1, 3, H', W') -> (H', W', 3)
        output = output_tensor.squeeze(0).permute(1, 2, 0)
        
        # è½¬æ¢å› numpy
        if self.half_precision:
            output = output.float()
        
        output = output.cpu().numpy()
        
        # æ ¹æ®ä»»åŠ¡è°ƒæ•´è¾“å‡ºèŒƒå›´
        if self.task != 'jpeg':
            output = output * 255.0
        
        output = output.clip(0, 255).astype(np.uint8)

        if self.device == 'cuda':
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass
        
        # æ›´æ–°ç»Ÿè®¡
        self.process_count += 1
        
        return output
    
    def process_batch(self, images_list):
        """
        æ‰¹é‡å¤„ç†å¤šå¼ å›¾åƒ
        
        Args:
            images_list: list of numpy arrays, each (H, W, 3), RGB, 0-255
        
        Returns:
            results: list of numpy arrays
        """
        if not images_list:
            return []
        
        # æ£€æŸ¥æ‰€æœ‰å›¾åƒå°ºå¯¸æ˜¯å¦ä¸€è‡´
        shapes = [img.shape for img in images_list]
        if len(set(shapes)) > 1:
            # å°ºå¯¸ä¸ä¸€è‡´ï¼Œé€ä¸ªå¤„ç†
            return [self.process(img) for img in images_list]
        
        # æ‰¹é‡å¤„ç†
        # é¢„å¤„ç†
        imgs = []
        for img_np in images_list:
            img = img_np.astype(np.float32)
            if self.task != 'jpeg':
                img = img / 255.0
            imgs.append(torch.from_numpy(img).permute(2, 0, 1))
        
        batch = torch.stack(imgs).to(self.device)
        if self.half_precision:
            batch = batch.half()
        
        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(batch)
        
        # åå¤„ç†
        results = []
        for output_tensor in outputs:
            output = output_tensor.permute(1, 2, 0)
            if self.half_precision:
                output = output.float()
            output = output.cpu().numpy()
            if self.task != 'jpeg':
                output = output * 255.0
            output = output.clip(0, 255).astype(np.uint8)
            results.append(output)
        
        self.process_count += len(images_list)
        
        return results
    
    def get_stats(self):
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'task': self.task,
            'device': self.device,
            'upscale': self.upscale,
            'half_precision': self.half_precision,
            'process_count': self.process_count,
            'training_patch_size': self.training_patch_size,
        }
        
        # æ·»åŠ ä»»åŠ¡ç‰¹å®šçš„å‚æ•°
        if self.task in ['denoise', 'gray_dn', 'color_dn']:
            stats['noise'] = self.noise
        if self.task in ['jpeg', 'jpeg_car', 'color_jpeg_car']:
            stats['jpeg'] = self.jpeg
        if self.tile is not None:
            stats['tile'] = self.tile
            stats['tile_overlap'] = self.tile_overlap
        
        return stats
    
    def __call__(self, image):
        """å…è®¸ç›´æ¥è°ƒç”¨å®ä¾‹"""
        return self.process(image)
    
    def __repr__(self):
        base = f"SwinIRProcessor(task={self.task}, upscale={self.upscale}, device={self.device}"
        
        if self.task in ['denoise', 'gray_dn', 'color_dn']:
            base += f", noise={self.noise}"
        if self.task in ['jpeg', 'jpeg_car', 'color_jpeg_car']:
            base += f", jpeg={self.jpeg}"
        if self.tile is not None:
            base += f", tile={self.tile}"
        
        base += f", processed={self.process_count})"
        return base


def test_swinir_processor():
    """æµ‹è¯• SwinIR å¤„ç†å™¨"""
    print("="*70)
    print("ğŸ§ª æµ‹è¯• SwinIR å¤„ç†å™¨")
    print("="*70)
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)
    print(f"\nğŸ“¸ æµ‹è¯•å›¾åƒå½¢çŠ¶: {test_image.shape}")
    
    # æµ‹è¯•ä¸åŒä»»åŠ¡ï¼ˆéœ€è¦ç›¸åº”çš„æ¨¡å‹æ–‡ä»¶ï¼‰
    # è¿™é‡Œåªæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨
    print("\nç¤ºä¾‹ç”¨æ³•:")
    print("""
    # 1. å½©è‰²å›¾åƒå»å™ªï¼ˆé»˜è®¤é…ç½®ï¼‰
    processor_dn = SwinIRProcessor(
        model_path='model_zoo/swinir/005_colorDN_DFWB_s128w8_SwinIR-M_noise15.pth',
        task='color_dn',      # é»˜è®¤
        noise=15,             # é»˜è®¤
        training_patch_size=128  # é»˜è®¤
    )
    output_dn = processor_dn.process(test_image)
    
    # 2. è¶…åˆ†è¾¨ç‡ï¼ˆ2xï¼‰
    processor_sr = SwinIRProcessor(
        model_path='model_zoo/swinir/001_classicalSR_DF2K_s64w8_SwinIR-M_x2.pth',
        task='classical_sr',
        upscale=2
    )
    output_sr = processor_sr.process(test_image)
    
    # 3. JPEG å‹ç¼©ä¼ªå½±å»é™¤
    processor_jpeg = SwinIRProcessor(
        model_path='model_zoo/swinir/006_CAR_DFWB_s126w7_SwinIR-M_jpeg40.pth',
        task='jpeg_car',
        jpeg=40               # é»˜è®¤
    )
    output_jpeg = processor_jpeg.process(test_image)
    
    # 4. ä½¿ç”¨ç“¦ç‰‡å¤„ç†å¤§å›¾åƒ
    processor_tile = SwinIRProcessor(
        model_path='model_zoo/swinir/001_classicalSR_DF2K_s64w8_SwinIR-M_x2.pth',
        task='classical_sr',
        upscale=2,
        tile=512,             # ä½¿ç”¨ 512x512 ç“¦ç‰‡
        tile_overlap=32       # é»˜è®¤é‡å 
    )
    output_tile = processor_tile.process(large_image)
    """)
    
    print("\n" + "="*70)


if __name__ == "__main__":
    test_swinir_processor()

